{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform, color\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import dataset, dataloader, Dataset\n",
    "from torchvision import transforms, utils"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "np.random.seed(42)\n",
    "all_dirs = glob(\"data/tiny-imagenet-200/train/*\")\n",
    "dirs = np.random.choice(all_dirs, 20)\n",
    "data = []\n",
    "for img_dir in dirs:\n",
    "    imgs_in_dir = glob(img_dir + \"/images/*.JPEG\")\n",
    "    name = img_dir.split(\"/\")[-1]\n",
    "    for img in imgs_in_dir:\n",
    "        data.append([name, img])\n",
    "        \n",
    "train_df = pd.DataFrame(data, columns=[\"name\", \"image\"])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "        name                                              image\n0  n04070727  data/tiny-imagenet-200/train/n04070727/images/...\n1  n04070727  data/tiny-imagenet-200/train/n04070727/images/...\n2  n04070727  data/tiny-imagenet-200/train/n04070727/images/...\n3  n04070727  data/tiny-imagenet-200/train/n04070727/images/...\n4  n04070727  data/tiny-imagenet-200/train/n04070727/images/...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>image</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>n04070727</td>\n      <td>data/tiny-imagenet-200/train/n04070727/images/...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>n04070727</td>\n      <td>data/tiny-imagenet-200/train/n04070727/images/...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>n04070727</td>\n      <td>data/tiny-imagenet-200/train/n04070727/images/...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>n04070727</td>\n      <td>data/tiny-imagenet-200/train/n04070727/images/...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>n04070727</td>\n      <td>data/tiny-imagenet-200/train/n04070727/images/...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, transform=None, alexnet_feature=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        # self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.alexnet_feature = alexnet_feature\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0] \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = io.imread(self.df.iloc[idx].image)\n",
    "        # image = read_image(self.df.iloc[idx].image).float()\n",
    "        # image = Image.open(self.df.iloc[idx].image)\n",
    "        if len(image.shape) == 2:\n",
    "            image = color.gray2rgb(image)\n",
    "        # adds another dimension to the image channel\n",
    "        name = f\"{self.df.iloc[idx].name}\"\n",
    "        sample = {'name': name, 'image': image}\n",
    "        # print(\"name\", sample['name'], sample['image'].shape)\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        if self.alexnet_feature:\n",
    "            sample['alex'] = self.alexnet_feature(sample['image'])\n",
    "\n",
    "        return sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image):\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: h x w x c\n",
    "        # torch image: c x h x w\n",
    "        # print(sample['name'], sample['image'].shape, sample['fix_map'].shape)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return torch.from_numpy(image).float()\n",
    "        # return {'name': sample['name'],\n",
    "        #         'image': torch.from_numpy(image).float(),\n",
    "        #         'fix_map': torch.from_numpy(fix_map).float()}\n",
    "\n",
    "\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size # tuple\n",
    "\n",
    "    def __call__(self, image):\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "        # h and w are swapped for fix_map because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        # fix_map = fix_map * [new_w / w, new_h / h]\n",
    "        return img\n",
    "\n",
    "class GetAlexConv(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_number, layer_name):\n",
    "        self.layer_number = layer_number\n",
    "        self.layer_name = layer_name\n",
    "\n",
    "        self.alex = models.alexnet(pretrained=True).to(device)\n",
    "        for param in self.alex.parameters():\n",
    "            param.requires_grad = False\n",
    "        child = list(self.alex.children())\n",
    "        conv_layer = child[self.layer_number[0]][self.layer_number[1]]\n",
    "        # conv_layer = child[0][10]\n",
    "        conv_layer.register_forward_hook(get_activation(self.layer_name))\n",
    "\n",
    "        # layer2 = 'maxpool5'\n",
    "        # max_five = child[0][12]\n",
    "        # max_five.register_forward_hook(get_activation(layer2))\n",
    "        # print(conv_five.weight.size())\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # return True\n",
    "        # batch = np.expand_dims(image, axis=0)\n",
    "\n",
    "        batch = image.to(device).unsqueeze(0)\n",
    "        self.alex(batch)\n",
    "        feature = activation[self.layer_name].squeeze(0)\n",
    "        return feature\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as fn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "\n",
    "transformed_dataset = ImageDataset(train_df,\n",
    "                                   transform=transforms.Compose([\n",
    "                                       Rescale((227, 227)),\n",
    "                                       # transforms.Resize(227),\n",
    "                                       # RandomCrop(224),\n",
    "                                       ToTensor(),\n",
    "                                       # transforms.ToTensor(),\n",
    "                                    #    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                   ]),\n",
    "                                    alexnet_feature=GetAlexConv(layer_number=[2, 6], layer_name='fc8'),\n",
    "                                   )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = Net().to(device)\n",
    "# print(net)\n",
    "# net(image).size()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "157"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................................................................................................................................................epoch 0 0.19716408060994117\n",
      "......................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        # name, image, fix_map = data\n",
    "        image = data['image'].to(device)\n",
    "        input_image = data['alex'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        print(\".\", end=\"\")\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(input_image)\n",
    "        # image_resize = fn.resize(image, size=192)\n",
    "        loss = criterion(outputs, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        # if i % 50 == 49:    # print every 2000 mini-batches\n",
    "        #     print('[%d, %5d] loss: %.3f' %\n",
    "        #           (epoch + 1, i + 1, running_loss / 50))\n",
    "        #     running_loss = 0.0\n",
    "\n",
    "    print(\"epoch\", epoch, running_loss / len(trainloader))\n",
    "\n",
    "print('Finished Training')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [
    {
     "data": {
      "text/plain": "1.3437084555625916"
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "alex = models.alexnet(pretrained=True).to(device)\n",
    "for param in alex.parameters():\n",
    "    param.requires_grad = False\n",
    "child = list(alex.children())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "ReLU(inplace=True)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child[2][2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}